{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size:40px;\"><center>Exercise II:<br> Model selection with MLPs\n",
    "</center></h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Short summary\n",
    "In this exercise you will: \n",
    "\n",
    "* train multi-layer perceptrons (MLPs) for both binary and multiple classification problems and a regression problem, and perform model selection to optimize validation performance\n",
    "\n",
    "You should write the report of the exercise within this notebook. The details of how to do that can be found below in section \"Writing the report\".\n",
    "\n",
    "**Deadline for submitting the report: See Canvas assignment.**\n",
    "\n",
    "## The data\n",
    "There are several datasets in this exercise. \n",
    "\n",
    "### syn2\n",
    "The *syn2* dataset represents a binary classification problem. The input data is 2D which allows for an easy visual inspection of the different classes and the decision boundary implemented by the network. The dataset is generated using random numbers each time you run the cell. This means that each time you generate the data it will be slightly different. You can control this by having a fixed *seed* to the random number generator. The cell \"PlotData\" will plot the *syn2* dataset.\n",
    "\n",
    "Note: This is the same dataset as in exercise 1.\n",
    "\n",
    "### regr2\n",
    "There *regr2* dataset represents a more complex synthetic regression problem than *regr1* from exercise 1. It has 6 inputs (independent variables) and one target variable (dependent variable). It is generated according to the following formula:  \n",
    "\n",
    "$\\qquad d = 2x_1 + x_2x_3^2 + e^{x_4} + 5x_5x_6 + 3\\sin(2\\pi x_6) + \\alpha\\epsilon$  \n",
    "    \n",
    "where $\\epsilon$ is added normally distributed noise and $\\alpha$ is a parameter controlling the size of the added noise. Variables $x_1,...,x_4$ are normally distributed with zero mean and unit variance, whereas $x_5, x_6$ are uniformly distributed ($[0,1]$). The target value $d$ has a non-linear dependence on ***x***.\n",
    "\n",
    "### Spiral data\n",
    "This is the \"famous\" spiral dataset that consists of two 2-D spirals, one for each class. The perfect classification boundary is also a spiral. The cell \"PlotData\" will plot this dataset.\n",
    "\n",
    "### Japanese vowels dataset\n",
    "*This data set is taken from the UCI Machine Learning Repository https://archive.ics.uci.edu/ml/datasets/Japanese+Vowels* In short, nine male speakers uttered two Japanese vowels /ae/ successively. For each utterance, a discrete times series was produced where each time point consists of 12 (LPC cepstrum) coefficients. The length of each time series was between 7-29. \n",
    "Here we treat each point of the time series as a feature (12 inputs). In total we have 9961\n",
    "data points which then has been divided into 4274 for training, 2275 for validation and 3412 for test. The original data files are provided as *ae.train* and *ae.test*. The task is now based on a single sample value of one of the speakers, determine which speaker it was. This is, in summary, a 9-class classification problem with 12 input values for each case.\n",
    "\n",
    "### Bioconcentration dataset\n",
    "*This data set is taken from the UCI Machine Learning Repository https://archive.ics.uci.edu/ml/datasets/QSAR+Bioconcentration+classes+dataset* In short, this is a dataset of manually-curated bioconcentration factors (BCF) for 779 chemicals used to determine the mechanisms of bioconcentration, i.e. to predict whether a chemical: (1) is mainly stored within lipid tissues, (2) has additional storage sites (e.g. proteins), or (3) is metabolized/eliminated. Data were randomly split into a training set of 584 compounds (75%) and a test set of 195 compounds (25%), preserving the proportion between the classes. The independent variables consist of 9 molecular descriptors. This is, in summary, a 3-class classification problem with 9 input values for each case.\n",
    "\n",
    "## The questions\n",
    "\n",
    "There are 10 questions in this exercise, in five different cells below. \n",
    "\n",
    "For questions 1-6, code is available that you can run directly or only need to make small modifications to. The first 3 questions deal with 2D binary classification problems. Here you will be able to see the boundary implemented by the different MLPs that you train. Questions 4-6 deal with training a regression network for the *regr2* dataset.\n",
    "\n",
    "For questions 7-10 we only provide parts of the code and you should add the rest. However, it is typically just a matter of paste and copy from the previous code cells (in a proper way). Question 7-8 deals with Japanese vowels classification problem; here your task is to come up with a model that optimizes the validation result. Question 9 is about the Bioconcentration dataset, and again you should come up with a good model. Finally, the last question is to find a model that can solve the spiral problem.\n",
    "\n",
    "## The different 'Cells'\n",
    "This notebook contains several cells with python code, together with the markdown cells (like this one) with only text. Each of the cells with python code has a \"header\" markdown cell with information about the code. The table below provides a short overview of the code cells. \n",
    "\n",
    "| #  |  CellName | CellType | Comment |\n",
    "| :--- | :-------- | :-------- | :------- |\n",
    "| 1 | Init | Needed | Sets up the environment|\n",
    "| 2 | MLP | Needed | Defines the MLP model |\n",
    "| 3 | Data | Needed | Defines the functions to generate the artificial datasets |\n",
    "| 4 | PlotData | Information | Plots the 2D classification datasets |\n",
    "| 5 | Statistics | Needed | Defines the functions that calculates various performance measures |\n",
    "| 6 | Boundary | Needed | Function that can show 2D classification boundaries | \n",
    "| 7 | Confusion | Needed | Functions that plots the confusion matrix | \n",
    "| 8 | Ex1 | Exercise | For question 1-3 |\n",
    "| 9 | Ex2 | Exercise | For question 4-6 |\n",
    "| 10 | Ex3 | Exercise | For question 7-8 |\n",
    "| 11 | Ex4 | Exercise | For question 9 |\n",
    "| 12 | Ex5 | Exercise | For question 10 |\n",
    "\n",
    "In order for you to start with the exercise you need to run all cells with the celltype \"Needed\". The very first time you start with this exercise we suggest that you enter each of the needed cells, read the cell instruction and run the cell. It is important that you do this in the correct order, starting from the top and work you way down the cells. Later when you have started to work with the notebook it may be easier to use the command \"Run All\" or \"Run all above\" found in the \"Cell\" dropdown menu.\n",
    "\n",
    "## Writing the report\n",
    "First the report should be written within this notebook. We have prepared the last cell in this notebook for you where you should write the report. The report should contain 4 parts:\n",
    "\n",
    "* Name:\n",
    "* Introduction: A **few** sentences where you give a small introduction of what you have done in the lab.\n",
    "* Answers to questions: For each of the questions provide an answer. It can be short answers or a longer ones depending on the nature of the questions, but try to be efficient in your writing.\n",
    "* Conclusion: Summarize your findings in a few sentences.\n",
    "\n",
    "It is important that you write the report in this last cell and **not** after each question! \n",
    "\n",
    "## Last but not least\n",
    "Have fun!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CellName: Init (#1)\n",
    "### CellType: Needed\n",
    "### Cell instruction: Initializing the libraries\n",
    "\n",
    "In the cell below, we import all the libraries that are needed for this exercises. \n",
    "\n",
    "Run the cell by entering into the cell and press \"CTRL Enter\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import metrics, regularizers, optimizers\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random as rn\n",
    "import scipy as sp\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import confusion_matrix, log_loss, classification_report\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CellName: MLP (#2)\n",
    "### CellType: Needed\n",
    "### Cell instruction: Defining the MLP model\n",
    "\n",
    "This cell defines the MLP model. There are a number of parameters that is needed to \n",
    "define a model. Here is a list of them: \n",
    "\n",
    "\n",
    "* inp_dim: the input dimension (integer)\n",
    "\n",
    "* n_nod: size of the network, eg [5] for a one hidden layer with 5 nodes and [5,3] for a two layer network with 5 and 3 hidden nodes each.\n",
    "\n",
    "* drop_nod: Dropout parameter for each hidden layer. You can specipty a single number that will be used for all hidden layers. If you want different dropout parameters for each hidden layer, then specify as a list. Example, for a two hidden layer network drop_nod = [0.5, 0.75] means drop hidden nodes with probability 0.5 and 0.75 for the first and the second hidden layer, respectively. Note that we do not use dropout on the input nodes! Also, a value of 0 means no dropout (i.e. zero probability of removing a node).\n",
    "\n",
    "* act_fun: the activation function. Most common are\n",
    "    * 'linear'\n",
    "    * 'relu'\n",
    "    * 'tanh'\n",
    "    * 'sigmoid'\n",
    "        \n",
    "* out_act_fun: the activation function for the output nodes. Most common are\n",
    "    * 'linear'\n",
    "    * 'sigmoid'\n",
    "    * 'softmax'\n",
    "    \n",
    "* opt_method: The error minimization method. Common choices\n",
    "    * 'SGD'\n",
    "    * 'Adam'\n",
    "    * 'Nadam'\n",
    "    * 'RMSprop'\n",
    "    \n",
    "* cost_fun: The error function used during training. There are three common ones\n",
    "    * 'mean_squared_error'\n",
    "    * 'binary_crossentropy'\n",
    "    * 'categorical_crossentropy'\n",
    "\n",
    "* lr_rate: The learning rate. \n",
    "\n",
    "* metric: The metric to use besides the loss. Common values\n",
    "    * accuracy\n",
    "    * mse\n",
    "\n",
    "* lambd: L2 regularization parameter\n",
    "\n",
    "* num_out: The number of output nodes\n",
    "\n",
    "Run the cell by entering into the cell and press \"CTRL Enter\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(inp_dim,\n",
    "            n_nod,\n",
    "            drop_nod,\n",
    "            act_fun = 'relu',\n",
    "            out_act_fun = 'sigmoid',\n",
    "            opt_method = 'Adam',\n",
    "            cost_fun = 'binary_crossentropy',\n",
    "            lr_rate = 0.01, \n",
    "            metric = 'accuracy',\n",
    "            lambd = 0.0, \n",
    "            num_out = None):\n",
    "    \n",
    "    lays = [inp_dim] + n_nod\n",
    "    \n",
    "    main_input = Input(shape=(inp_dim,), dtype='float32', name='main_input')\n",
    "    \n",
    "    X = main_input\n",
    "    for i, nod in enumerate(n_nod):\n",
    "        X = Dense(nod, \n",
    "                  activation = act_fun,\n",
    "                  kernel_regularizer=regularizers.l2(lambd))(X)\n",
    "        if type(drop_nod) is list: \n",
    "            X = Dropout(drop_nod[i])(X)\n",
    "        else: \n",
    "            X = Dropout(drop_nod)(X)\n",
    "        \n",
    "    output = Dense(num_out, activation = out_act_fun )(X)\n",
    "    \n",
    "    method = getattr(optimizers, opt_method)\n",
    "    \n",
    "    model =  Model(inputs=[main_input], outputs=[output])\n",
    "    model.compile(optimizer = method(learning_rate = lr_rate),\n",
    "                  loss = cost_fun,\n",
    "                  metrics=[metric])       \n",
    "    \n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CellName: Data (#3)\n",
    "### CellType: Needed\n",
    "### Cell instruction: Defining synthetic data sets\n",
    "\n",
    "This cell defines the different synthetic data sets. It also provides functions for reading the Vowles dataset, the Bioconcentration dataset and the Spiral data. The last function is used for standardization of the data. \n",
    "\n",
    "Run the cell by entering into the cell and press \"CTRL Enter\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def syn2(N):\n",
    "    \n",
    "    global seed\n",
    "     \n",
    "    x = np.empty(shape=(N,2), dtype = np.float32)  \n",
    "    d = np.empty(shape=(N,), dtype = np.float32) \n",
    "    N1 = int(N/2)\n",
    "\n",
    "    # Positive samples\n",
    "    x[:N1,:] = 0.8 + np.random.normal(loc=.0, scale=1., size=(N1,2))\n",
    "\n",
    "    # Negative samples \n",
    "    x[N1:,:] = -.8 + np.random.normal(loc=.0, scale=1., size=(N-N1,2))\n",
    "    \n",
    "    # Target\n",
    "    d[:N1] = np.ones(shape=(N1,))\n",
    "    d[N1:] = np.zeros(shape=(N-N1,))\n",
    "\n",
    "    return x,d\n",
    "\n",
    "\n",
    "def regr2(N, v=0):\n",
    "\n",
    "    global seed\n",
    "\n",
    "    x = np.empty(shape=(N,6), dtype = np.float32)  \n",
    "    \n",
    "    uni = lambda n : np.random.uniform(0,1,n)\n",
    "    norm = lambda n : np.random.normal(0,1,n)\n",
    "    noise =  lambda  n : np.random.normal(0,1,n)\n",
    "    \n",
    "    \n",
    "    for i in range(4):\n",
    "        x[:,i] = norm(N)\n",
    "    for j in [4,5]:\n",
    "        x[:,j] = uni(N)\n",
    "    \n",
    "    d =   2*x[:,0] + x[:,1]* x[:,2]**2 + np.exp(x[:,3]) + \\\n",
    "            5*x[:,4]*x[:,5]  + 3*np.sin(2*np.pi*x[:,5])\n",
    "    std_signal = np.std(d)\n",
    "    d = d + v * std_signal * noise(N)\n",
    "        \n",
    "    return x, d\n",
    "\n",
    "def twospirals(n_points, turns = 3, noise=0.5):\n",
    "    \"\"\"\n",
    "     Returns the two spirals dataset.\n",
    "    \"\"\"\n",
    "    n = (np.random.rand(n_points,1)*0.95+0.05) * turns * (2*np.pi)\n",
    "    d1x = -np.cos(n)*n + np.random.rand(n_points,1) * noise\n",
    "    d1y = np.sin(n)*n + np.random.rand(n_points,1) * noise\n",
    "    return (np.vstack((np.hstack((d1x,d1y)), np.hstack((-d1x,-d1y)))), \n",
    "            np.hstack((np.zeros(n_points),np.ones(n_points))))\n",
    "\n",
    "def vowels():\n",
    "    \n",
    "    def pre_proc(file_name):\n",
    "        block = []\n",
    "        x = []\n",
    "    \n",
    "        with open(file_name) as file:\n",
    "            for line in file:    \n",
    "                if line.strip():\n",
    "                    numbers = [float(n) for n in line.split()]\n",
    "                    block.append(numbers)\n",
    "                else:\n",
    "                    x.append(block)\n",
    "                    block = []\n",
    "                \n",
    "        ################################\n",
    "        x = [np.asarray(ar) for ar in x]    \n",
    "        return x\n",
    "\n",
    "    x_trn = pre_proc('ae.train')\n",
    "    x_tst = pre_proc('ae.test')\n",
    "\n",
    "    \n",
    "    ############## LABELS###########\n",
    "    chunk1 = list(range(30,270, 30))\n",
    "    d_trn = []\n",
    "    person = 0\n",
    "\n",
    "    for i, block in enumerate(x_trn):\n",
    "        if i in chunk1:\n",
    "            person += 1\n",
    "        d_trn.extend([person]*block.shape[0])\n",
    "        \n",
    "    chunk2 = [31,35,88,44,29,24,40,50,29]\n",
    "    chunk2 = np.cumsum(chunk2)\n",
    "    d_tst = []\n",
    "    person = 0\n",
    "    for i, block in enumerate(x_tst):\n",
    "        if i in chunk2:\n",
    "            person += 1\n",
    "        d_tst.extend([person]*block.shape[0])\n",
    "\n",
    "    x_trn = np.vstack(x_trn)\n",
    "    x_tst = np.vstack(x_tst)\n",
    "    \n",
    "    ## Split into train, validation and test\n",
    "    num_classes = 9\n",
    "    d_trn = keras.utils.to_categorical(d_trn, num_classes)\n",
    "    d_tst = keras.utils.to_categorical(d_tst, num_classes)\n",
    "\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    x_tst, x_val, d_tst, d_val = train_test_split(x_tst, d_tst, test_size=0.4, random_state=41)\n",
    "\n",
    "    return x_trn, d_trn, x_val, d_val, x_tst, d_tst\n",
    "\n",
    "def bcf():\n",
    "\n",
    "    bcf_trn = pd.read_csv(\"Grisoni_trn.csv\", delimiter='\\t')\n",
    "    bcf_tst = pd.read_csv(\"Grisoni_tst.csv\", delimiter='\\t')\n",
    "\n",
    "    x_trn = bcf_trn.iloc[:,3:12].values.astype(np.float32)\n",
    "    d_trn = bcf_trn.iloc[:,12].values.astype(np.float32) - 1.0\n",
    "    d_trn = keras.utils.to_categorical(d_trn, 3)\n",
    "\n",
    "    x_tst = bcf_tst.iloc[:,3:12].values.astype(np.float32)\n",
    "    d_tst = bcf_tst.iloc[:,12].values.astype(np.float32) - 1.0\n",
    "    d_tst = keras.utils.to_categorical(d_tst, 3)\n",
    "    \n",
    "    return x_trn, d_trn, x_tst, d_tst\n",
    "\n",
    "def standard(x):\n",
    "    return np.mean(x, axis=0) , np.std(x, axis=0)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CellName: PlotData (#4)\n",
    "### CellType: Information\n",
    "### Cell instruction: Plotting the data\n",
    "\n",
    "Here we just generate 100 cases for syn2 and the spiral dataset and plot them. \n",
    "\n",
    "Run the cell by entering into the cell and press \"CTRL Enter\". \n",
    "\n",
    "**Note!** This cell is not needed for the actual exercises, it is just to visualize the four different 2D synthetic classification data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# seed = 0 means random, seed > 0 means fixed\n",
    "seed = 0\n",
    "np.random.seed(seed) if seed else None\n",
    "\n",
    "x,d = syn2(100)\n",
    "plt.figure(2)\n",
    "plt.scatter(x[:,0],x[:,1], c=d)\n",
    "\n",
    "x,d = twospirals(500, 3, 0)\n",
    "plt.figure(4)\n",
    "plt.scatter(x[:,0],x[:,1], c=d)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CellName: Statistics (#5)\n",
    "### CellType: Needed\n",
    "### Cell instruction: Present result for both classification and regression problems\n",
    "\n",
    "This cell defines two functions that we are going to call using a trained model to calculate both error and performance measures. \n",
    "\n",
    "Run the cell by entering into the cell and press \"CTRL Enter\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stats_class(x = None, d = None, label = 'Training', model = None):\n",
    "    \"\"\"\n",
    "    input :  \n",
    "             x = input\n",
    "             d = target\n",
    "             label = \"Provided text string\"\n",
    "             model = the model\n",
    "             \n",
    "    output : \n",
    "             sensitivity = fraction of correctly classified positive cases\n",
    "             specificity = fraction of correctly classified negative cases\n",
    "             accuracy = fraction of correctly classified cases\n",
    "             loss = typically the cross-entropy error\n",
    "    \"\"\"\n",
    "    \n",
    "    def binary(y1):\n",
    "        y1[y1>.5] = 1.\n",
    "        y1[y1<= .5] = 0.        \n",
    "        return y1\n",
    "\n",
    "    d_pr = model.predict(x, batch_size = x.shape[0], verbose=0).reshape(d.shape)\n",
    "                \n",
    "    nof_p, tp, nof_n, tn = [np.count_nonzero(k) for k in [d==1, d_pr[d==1.] > 0.5, d==0, d_pr[d==0.]<= 0.5]]\n",
    "    \n",
    "    sens = tp / nof_p\n",
    "    spec = tn / nof_n\n",
    "    acc = (tp + tn) / (len(d))\n",
    "    loss = model.evaluate(x, d, batch_size =  x.shape[0], verbose=0)\n",
    "                \n",
    "    A = ['Accuracy', 'Sensitivity', 'Specificity', 'Loss']\n",
    "    B = [acc, sens, spec, loss[0]]\n",
    "    \n",
    "    print('\\n','#'*10,'STATISTICS for {} Data'.format(label), '#'*10, '\\n')\n",
    "    for i in range(len(A)):\n",
    "        print('{:15} {:.4f}'.format(A[i], B[i]))\n",
    "\n",
    "    return print('\\n','#'*50)\n",
    "\n",
    "def stats_reg(x = None, d = None, label = 'Training', model = None):\n",
    "    \"\"\"\n",
    "    input :  \n",
    "             x = input\n",
    "             d = target\n",
    "             label = \"Provided text string\"\n",
    "             model = the model\n",
    "             \n",
    "    output : \n",
    "             MSE = mean squeared error between target and predictions\n",
    "             CorrCoeff = correlation coefficient for the scatter between predictions and target values\n",
    "    \"\"\"\n",
    "    d_pred = model.predict(x, batch_size = x.shape[0], verbose=0).reshape(d.shape)\n",
    "\n",
    "    mse = np.mean((d - d_pred)**2)\n",
    "    pcorr = np.corrcoef(d, d_pred)[1,0]\n",
    "    \n",
    "    A = ['MSE', 'CorrCoeff']\n",
    "    B = [mse, pcorr]\n",
    "    \n",
    "    print('\\n','#'*10,'STATISTICS for {} Data'.format(label), '#'*10, '\\n')\n",
    "    for i in range(len(A)):\n",
    "        print('{:15} {:.10f}'.format(A[i], B[i]))\n",
    "\n",
    "    return print('\\n','#'*50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CellName: Confusion (#6)\n",
    "### CellType: Needed\n",
    "### Cell Instruction: Plot the confusion matrix\n",
    "\n",
    "This cell defines the functions need to plot a confusion matrix. A confusion matrix is a summary of the predictions made by a classifier. Each column of the matrix represents the instances of the predicted class while each row represents the instances of the actual class. The function 'plot_confusion_matrix' does the actual plotting, while the 'make_cm_plot' is the one that should be called from the user. See example of usage in the exercises. \n",
    "\n",
    "Run the cell by entering into the cell and press \"CTRL Enter\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm,\n",
    "                          target_names,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=None,\n",
    "                          normalize=True):\n",
    "    \n",
    "    import itertools\n",
    "\n",
    "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
    "    misclass = 1 - accuracy\n",
    "\n",
    "    if cmap is None:\n",
    "        cmap = plt.get_cmap('Blues')\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    #plt.ylim([-0.5, cm.shape[0]-0.5])\n",
    "\n",
    "    if target_names is not None:\n",
    "        tick_marks = np.arange(len(target_names))\n",
    "        plt.xticks(tick_marks, target_names, rotation=45)\n",
    "        plt.yticks(tick_marks, target_names)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "\n",
    "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if normalize:\n",
    "            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        else:\n",
    "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n",
    "    plt.show()\n",
    "    \n",
    "def make_cm_plot(model,\n",
    "            inp,\n",
    "            trg,\n",
    "            num_classes,\n",
    "            label='Test data'):\n",
    "    \n",
    "    print('\\n','#'*10,'Result for {}'.format(label), '#'*10, '\\n')\n",
    "\n",
    "    y = model.predict(inp, verbose=0 )\n",
    "    print('log_loss:   ', '{:.4f}'.format(log_loss(trg, y, eps=1e-15)))\n",
    "\n",
    "    d_class = trg.argmax(axis=1)\n",
    "    y_class = y.argmax(axis=1)\n",
    "    print('accuracy:   ', '{:.4f}'.format((y_class==d_class).mean()), '\\n')\n",
    "\n",
    "    class_names = ['class {}'.format(i+1) for i in range(num_classes)]\n",
    "    print(classification_report(d_class, y_class, target_names=class_names))\n",
    "\n",
    "    confuTst = confusion_matrix(d_class, y_class)\n",
    "    plot_confusion_matrix(cm           = confuTst, \n",
    "                          normalize    = False,\n",
    "                          target_names = class_names,\n",
    "                          title        = \"Confusion Matrix\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CellName: Boundary (#7)\n",
    "### CellType: Needed\n",
    "### Cell Instruction: Decision boundary\n",
    "\n",
    "This cell defines the function to plot the decision boundary for a 2D input binary MLP classifier. \n",
    "\n",
    "Run the cell by entering into the cell and press \"CTRL Enter\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_b(X, Y1, model ):\n",
    "    \n",
    "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "    # grid stepsize\n",
    "    h = 0.025\n",
    "\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()], verbose=0)\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    Z[Z>.5] = 1\n",
    "    Z[Z<= .5] = 0\n",
    "\n",
    "    Y_pr = model.predict(X, batch_size = X.shape[0], verbose=0).reshape(Y1.shape)\n",
    "  \n",
    "    Y = np.copy(Y1)\n",
    "    Y_pr[Y_pr>.5] = 1\n",
    "    Y_pr[Y_pr<= .5] = 0\n",
    "    Y[(Y!=Y_pr) & (Y==0)] = 2\n",
    "    Y[(Y!=Y_pr) & (Y==1)] = 3\n",
    "    \n",
    "    \n",
    "    plt.figure()\n",
    "    #plt.contourf(xx, yy, Z, cmap=plt.cm.PRGn, alpha = .9) \n",
    "    plt.contour(xx, yy, Z, cmap=plt.cm.Paired)\n",
    "    \n",
    "    \n",
    "    plt.scatter(X[:, 0][Y==1], X[:, 1][Y==1], marker='+', c='k')\n",
    "    plt.scatter(X[:, 0][Y==0], X[:, 1][Y==0], marker='o', c='k')\n",
    "       \n",
    "    plt.scatter(X[:, 0][Y==3], X[:, 1][Y==3], marker = '+', c='r')   \n",
    "    plt.scatter(X[:, 0][Y==2], X[:, 1][Y==2], marker = 'o', c='r')\n",
    "    \n",
    "    \n",
    "    plt.ylabel('x2')\n",
    "    plt.xlabel('x1')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "End of \"Needed\" and \"Information\" cells. Below are the cells for the actual exercise.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CellName: Ex1 (#8)\n",
    "### CellType: Exercise\n",
    "### Cell instruction: Instruction for questions 1-3\n",
    "\n",
    "The cell below should be used for questions 1-3. For question 1 you can run the cell as it is (i.e. CTRL-Return). For the other questions you need to modify the cell in order to change hyperparameters etc. \n",
    "\n",
    "From now on we will talk about *performance*! It can be performance of a trained model on the training dataset or the performance on the validation dataset. What do we mean by performance?  For classification problems we will provide 4 different measurements as returned by a call to the *stats_class* function. They are:\n",
    "* Sensitivity = fraction of correctly classified \"1\" cases\n",
    "* Specificity = fraction of correctly classified \"0\" cases\n",
    "* Accuracy = fraction of correctly classified cases\n",
    "* loss = cross-entropy error (so low loss means good performance!)\n",
    "\n",
    "For the questions in this exercise, accuracy is an appropriate performance measure.\n",
    "\n",
    "#### Question 1, single-node validation performance\n",
    "Here you are going to train a classifier for the *syn2* dataset. You are also going to use a validation dataset as an estimate of the *true* performance. Since we generate these datasets we can allow for a relatively large validation dataset in order to get a more accurate estimation of *true* performance. The default value in the cell is to generate 1000 validation data points. \n",
    "\n",
    "Now, use *syn2* (100 training data points) and train a *linear* MLP to separate the two classes, i.e. use a single hidden node. **What is the performance you get on the validation dataset?** \n",
    "\n",
    "**Note:** Use a fixed random seed for this exercise since you will compare with runs in the next questions.\n",
    "\n",
    "**Hint:** Remember from the first computer exercise that you should average over a few trained models.\n",
    "\n",
    "#### Question 2, improving training performance\n",
    "You are now going to train this model to a high training accuracy! By increasing the number of hidden nodes we should be able to get better and better training performance. **(a) How many hidden nodes do you need to reach an accuracy >95% on your training dataset?** **(b) What is the performance on the validation data set?**\n",
    "\n",
    "**Hint:** Remember from the first computer exercise that overtraining often means finding a good local minimum of the loss function, which may require some tuning of the hyperparameters that control the training. This means that you may have to change the learning rate, batch size and the number of epochs. Since the *Adam* method is usually better than the vanilla *stochastic gradient descent*, it is used in the cells below as the default minimizer. \n",
    "\n",
    "#### Question 3, optimizing validation performance\n",
    "However, we are almost always interested in optimal *validation* performance. You should now find the number of hidden nodes that optimize the validation performance. **(a) What is the optimal number of hidden nodes for the syn2 dataset in order to maximize your validation performance?** **(b) Try to give an explanation for the number you obtained.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# seed = 0 means random, seed > 0 means fixed\n",
    "seed = 3\n",
    "np.random.seed(seed) if seed else None\n",
    "\n",
    "# Generate training data\n",
    "x_trn, d_trn = syn2(100)\n",
    "x_val, d_val = syn2(1000)\n",
    "\n",
    "# Standardization of inputs\n",
    "mu, std = standard(x_trn)\n",
    "x_trn = (x_trn - mu)/std\n",
    "x_val = (x_val - mu)/std\n",
    "\n",
    "#### \n",
    "# Define the network, cost function and training settings\n",
    "INPUT = {'inp_dim': x_trn.shape[1],         \n",
    "         'n_nod': [1],                      # architecture\n",
    "         'drop_nod': 0.0,                   # dropout specification\n",
    "         'act_fun': 'tanh',                 # activation functions for the hidden layer\n",
    "         'out_act_fun': 'sigmoid',          # output activation function\n",
    "         'opt_method': 'Adam',              # minimization method\n",
    "         'cost_fun': 'binary_crossentropy', # error function\n",
    "         'lr_rate': 0.05 ,                  # learning rate\n",
    "         'num_out' : 1 }                    # if binary --> 1 |  regression--> num inputs | multi-class--> num of classes\n",
    "number_epochs = 500\n",
    "minibatch_size = 25\n",
    "####    \n",
    "\n",
    "# Get the model\n",
    "model_ex1 = mlp(**INPUT)\n",
    "\n",
    "# Print a summary of the model\n",
    "model_ex1.summary()\n",
    "\n",
    "# Train the model\n",
    "estimator_ex1 = model_ex1.fit(x_trn, d_trn,\n",
    "                      validation_data=(x_val, d_val),\n",
    "                      epochs=number_epochs,\n",
    "                      batch_size=minibatch_size,\n",
    "                      verbose=0)\n",
    "\n",
    "# Call the stats function to print out statistics for classification problems\n",
    "stats_class(x_trn, d_trn, 'Training', model_ex1)\n",
    "stats_class(x_val, d_val, 'Validation', model_ex1)\n",
    "\n",
    "# Training history\n",
    "plt.figure()\n",
    "plt.ylabel('Loss / Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "for k in estimator_ex1.history.keys():\n",
    "    plt.plot(estimator_ex1.history[k], label = k) \n",
    "plt.legend(loc='best')\n",
    "plt.show()\n",
    "\n",
    "# Show the decision boundary for the training dataset\n",
    "decision_b(x_trn, d_trn, model_ex1)\n",
    "\n",
    "# If you uncomment this one you will see how the decsion boundary is with respect to the validation data\n",
    "#decision_b(x_val, d_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CellName: Ex2 (#9)\n",
    "### CellType: Exercise\n",
    "### Cell instruction: Instruction for questions 4-6\n",
    "\n",
    "Now we are going to look at a regression problem. The data as described above (*regr2*) consists of 6 inputs (features) and one output (target) value. As in the previous exercise, a new data set is generated each time you call the *regr2* function. To get exactly the same data set between different calls, use a fixed seed. For this problem we can control the amount of noise added to the target value. We are going to use a relatively small training dataset (250) and a larger validation dataset (1000) to get a more robust estimation of the generalization performance, and 0.4 units of noise. For regression problems we also need new performance measures. The *stats_reg* function will give you two such measures:\n",
    "* MSE = mean squared error (low error means good performance)\n",
    "* CorrCoeff = Pearson correlation coefficient for the scatter plot between predicted and true values.\n",
    "\n",
    "The cell below can be used as a template for all questions regarding this regression problem.\n",
    "\n",
    "#### Question 4, optimizing regression performance\n",
    "*Model selection based on the number of hidden nodes (in a single hidden layer).* Find the number of hidden nodes that gives best validation performance. **How many hidden nodes gives the best validation performance?** **What is the best validation MSE (or correlation coefficient) you get?**\n",
    "\n",
    "**Hint:** A good strategy is to start with a \"small\" model and increase the number of hidden nodes and monitor the validation result.\n",
    "\n",
    "#### Question 5, improving generalization with L2 regularization\n",
    "*Model selection based on L2 (weight decay).* Instead of using the number of hidden nodes to control the complexity we can use a regularization term added to the error function. You are going to control the complexity by adding a *L2* regularizer (see the \"INPUT\" dictionary in the cell). For the L2 regularization to make sense we need a start model that is capable being overtrained. The suggestion is to use at least twice as many hidden nodes for this question compared to what you found in Q4. You should modify the *L2* value until you find the optimal validation performance. **(a) Present your optimal model (L2 value and number of hidden nodes) and the validation performance.** **(b) Do you obtain a better result compared to Q4?**\n",
    "\n",
    "**Hint:** When you test different values for a hyperparameter, it usually makes more sense to multiply with a constant factor than to add a constant term. For example, if you test five values in the range from 0.1 to 10, the values {0.1, 0.3, 1, 3, 10} are usually a better choice than {0.1, 2.5, 5, 7.5, 10}.\n",
    "\n",
    "#### Question 6, improving generalization with dropout\n",
    "*Model selection based on dropout.* Instead of using the *L2* regularizer we can use dropout. In short, repeat Q5, but use the *dropout* parameter instead. **(a) Present your optimal model (dropout value and number of hidden nodes) and the validation performance.** **(b) Do you obtain a better result compared to Q4/Q5?** \n",
    "\n",
    "**Hint:** Using dropout may require even more hidden nodes to start with! \n",
    "\n",
    "#### Extra question\n",
    "The extra questions is provided if you have extra time. **These question are not required for the course and do not influence any grading.** \n",
    "\n",
    "Repeat Q4-Q6 using two hidden layers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# seed = 0 means random, seed > 0 means fixed\n",
    "seed = 10\n",
    "np.random.seed(seed) if seed else None\n",
    "\n",
    "# Generate training and validation data\n",
    "x_trn, d_trn = regr2(250, 0.4)\n",
    "x_val, d_val = regr2(1000, 0.4)\n",
    "\n",
    "# Standardization of both inputs and targets\n",
    "mu, std = standard(x_trn)\n",
    "x_trn = (x_trn - mu)/std\n",
    "x_val = (x_val - mu)/std\n",
    "\n",
    "mu, std = standard(d_trn)\n",
    "d_trn = (d_trn - mu) / std\n",
    "d_val = (d_val - mu) / std\n",
    "\n",
    "#### \n",
    "# Define the network, cost function and training settings\n",
    "INPUT = {'inp_dim': x_trn.shape[1],         \n",
    "         'n_nod': [1],                  \n",
    "         'drop_nod': 0.0,                \n",
    "         'act_fun': 'tanh',             \n",
    "         'out_act_fun': 'linear',      \n",
    "         'opt_method': 'Adam',         \n",
    "         'cost_fun': 'mse',           \n",
    "         'lr_rate': 0.025,            \n",
    "         'metric': 'mse',              \n",
    "         'lambd' : 0.0,             \n",
    "         'num_out' : 1 }    \n",
    "number_epochs = 500\n",
    "minibatch_size = 50\n",
    "####    \n",
    "\n",
    "# Get the model\n",
    "model_ex2 = mlp(**INPUT)\n",
    "\n",
    "# Print a summary of the model\n",
    "model_ex2.summary()\n",
    "    \n",
    "# Train the model\n",
    "estimator_ex2 = model_ex2.fit(x_trn, d_trn,\n",
    "                      validation_data=(x_val,d_val),\n",
    "                      epochs=number_epochs,\n",
    "                      batch_size=minibatch_size,\n",
    "                      verbose=0)\n",
    "\n",
    "# Call the stat_reg to get MSE and correlation coefficiant for the scatter plot\n",
    "stats_reg(x_trn, d_trn, 'Training', model_ex2)\n",
    "stats_reg(x_val, d_val, 'Validation', model_ex2)\n",
    "\n",
    "# Scatter plots of predicted and true values\n",
    "pred_trn = model_ex2.predict(x_trn, verbose=0).reshape(d_trn.shape)\n",
    "pred_val = model_ex2.predict(x_val, verbose=0).reshape(d_val.shape)\n",
    "plt.figure()\n",
    "plt.plot(d_trn, pred_trn, 'g*', label='Predict vs True (Training)')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(d_val, pred_val, 'b*', label='Predict vs True (Validation)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Training history\n",
    "plt.figure()\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "for k in ['loss', 'val_loss']:\n",
    "    plt.plot(estimator_ex2.history[k], label = k) \n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CellName: Ex3 (#10)\n",
    "### CellType: Exercise\n",
    "### Cell instruction: Instruction for questions 7-8\n",
    "\n",
    "For this exercise you are given a classification problem with fixed training-, validation- and test datasets. The data is the Japanse vowels dataset described in the first cell. Your task is to do model selection, coming up with your optimal MLP architecture together with the hyperparameters you used. We provide less code here: normalization of the input data and the definition of the MLP is missing. You need to provide that on your own.\n",
    "\n",
    "#### Question 7, create MLP for binary classification\n",
    "**(a) Present an MLP with associated hyperparameters that maximizes the validation performance, and state the training, validation and test performance you obtained.**\n",
    "**(b) Present your code.**\n",
    "\n",
    "**Hint 1:** \n",
    "Remember to normalize the input data.\n",
    "\n",
    "**Hint 2:** \n",
    "This problem is a 9-class classification problem, meaning that you should use a specific output activation function (*out_act_fun*) and a specific loss/error function (*cost_fun*).\n",
    "\n",
    "**Hint 3:**\n",
    "Place a line with three tildes (\\~\\~\\~) or backticks(\\´\\´\\´) above and below your code in the report:\n",
    "~~~\n",
    "    for a in ['Hello', 'World']:\n",
    "        print(a)\n",
    "~~~\n",
    "\n",
    "#### Question 8, model selection criteria\n",
    "The typical goal is to have a high accuracy (i.e. the fraction of correctly classified cases). During training we typically monitor possible overtraining by looking at the *loss* of the validation data, since this is the error used during training. However, one can have a situation where the validation loss increases during training but the accuracy stays constant. **Why can this happen?** **Given this situation, what would be your criteria to select the best model?** \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Load the data\n",
    "x_trn, d_trn, x_val, d_val, x_tst, d_tst = vowels()\n",
    "\n",
    "# YOUR CODE FOR NORMALIZATION\n",
    "\n",
    "num_classes = 9\n",
    "\n",
    "# YOUR CODE THAT DEFINES THE MLP\n",
    "# INPUT = \n",
    "\n",
    "# Get the model\n",
    "model_vowels = mlp(**INPUT)\n",
    "\n",
    "# Print a summary of the model\n",
    "model_vowels.summary()\n",
    "\n",
    "# Train the model\n",
    "estimator_vowels = model_vowels.fit(x_trn, d_trn,\n",
    "                      validation_data=(x_val, d_val),\n",
    "                      epochs=number_epochs,\n",
    "                      batch_size=minibatch_size,\n",
    "                      verbose=0)\n",
    "\n",
    "# Plot the learning curves\n",
    "plt.figure()\n",
    "plt.ylabel('Loss / Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "for k in estimator_vowels.history.keys():\n",
    "    plt.plot(estimator_vowels.history[k], label = k) \n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# plot the confusion matrix\n",
    "make_cm_plot(model_vowels, x_trn, d_trn, num_classes, 'Training data')\n",
    "make_cm_plot(model_vowels, x_val, d_val, num_classes, 'Validation data')\n",
    "make_cm_plot(model_vowels, x_tst, d_tst, num_classes, 'Test data')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CellName: Ex4 (#11)\n",
    "### CellType: Exercise\n",
    "### Cell instruction: Instruction for question 9\n",
    "\n",
    "For this exercise you are given a classification problem with a fixed training and test dataset. The data is the Bioconcentraion dataset described in the first cell. Your task is to do model selection, coming up with your optimal MLP architecture together with the hyperparameters you used. We do not provide any python code for this question, only the small part that reads the data (next code cell).\n",
    "\n",
    "#### Question 9, create MLP for multi-class problem\n",
    "**(a) Present an MLP with associated hyperparameters that maximizes the validation performance and give the test performance you obtained.** For this classification task there are not so many cases of class 2. In the training data there is: class 1: 345 cases, class 2: 48 cases, and class 3: 191 cases. One can end upp with situations that the network does not at all learn how to detect cases of class 2. **(b) What performance measure(s) do you think is(are) relevant when you select the optimal model for this problem?**  \n",
    "\n",
    "**Hint 1:** \n",
    "Remember to normalize input data.\n",
    "\n",
    "**Hint 2:** \n",
    "Since there is no defined validation data set you need to split your original training data into training and validation data. You can use *sklearn.model_selection.train_test_split* or *sklearn.model_selection.KFold* to accomplish that, where the latter method does k-fold crossvalidation splits.\n",
    "\n",
    "**Hint 3:** \n",
    "This problem is a 3-class classification problem, meaning that you should use a specific output activation function (*out_act_fun*) and a specific loss/error function (*cost_fun*).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# seed = 0 means random, seed > 0 means fixed\n",
    "seed = 0\n",
    "np.random.seed(seed) if seed else None\n",
    "\n",
    "# Load Bioconcentration training and test data\n",
    "x_trn, d_trn, x_tst, d_tst = bcf()\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CellName: Ex5 (#12)\n",
    "### CellType: Exercise\n",
    "### Cell instruction: Instruction for question 10\n",
    "\n",
    "For this exercise the task is to train a binary classifier for the spiral problem. The aim is to get *zero* classification error for the training data (there is no test or validation data) with a model that is *as small as possible* in terms of the number of trainable parameters. Also plot the boundary to see if it resembles a spriral. To pass this question you should at least try! The data is randomly generated and we suggest using at least 1000 data points to get \"good\" spirals.\n",
    "\n",
    "#### Question 10, minimize network size\n",
    "**Train a classifier for the spiral problem with the aim of zero classification error with as small as possible model. Report the model you used.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# seed = 0 means random, seed > 0 means fixed\n",
    "seed = 0\n",
    "np.random.seed(seed) if seed else None\n",
    "    \n",
    "# Generate training data\n",
    "x_trn, d_trn = twospirals(1500, 3, 0)\n",
    "\n",
    "mu, std = standard(x_trn)\n",
    "x_trn = (x_trn - mu)/std\n",
    "\n",
    "#### \n",
    "# Define the network, cost function and training settings\n",
    "INPUT = {'inp_dim': x_trn.shape[1],\n",
    "         'n_nod': [5],\n",
    "         'drop_nod': 0.0,\n",
    "         'act_fun': 'tanh',\n",
    "         'out_act_fun': 'sigmoid',\n",
    "         'opt_method': 'Adam',\n",
    "         'cost_fun': 'binary_crossentropy',\n",
    "         'lr_rate': 0.01,\n",
    "         'num_out' : 1 }\n",
    "number_epochs = 2000\n",
    "minibatch_size = 100\n",
    "####    \n",
    "\n",
    "# Get the model\n",
    "model_ex6 = mlp(**INPUT)\n",
    "\n",
    "# Print a summary of the model\n",
    "model_ex6.summary()\n",
    "\n",
    "# Train the model\n",
    "estimator_ex6 = model_ex6.fit(x_trn, d_trn,\n",
    "                      epochs=number_epochs,\n",
    "                      batch_size=minibatch_size,\n",
    "                      verbose=0)\n",
    "\n",
    "# Call the stats function to print out statistics for the training\n",
    "stats_class(x_trn, d_trn, 'Training', model_ex6)\n",
    "\n",
    "# Training history\n",
    "plt.figure()\n",
    "plt.ylabel('Loss / Accuracy')\n",
    "plt.xlabel('epoch')\n",
    "for k in estimator_ex6.history.keys():\n",
    "    plt.plot(estimator_ex6.history[k], label = k) \n",
    "plt.legend(loc='best')\n",
    "\n",
    "# Show the decision boundary\n",
    "decision_b(x_trn, d_trn, model_ex6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The report!\n",
    "We have added intructions inside this report template. As you write your report, remove the instructions.\n",
    "\n",
    "## Name\n",
    "\n",
    "## Introduction\n",
    "A few sentences about the overall theme of the exercise.\n",
    "\n",
    "## Answers to questions\n",
    "Provide enough information to clarify the meaning of your answers, so that they can be understood by someone who does not scroll up and read the entire instruction.\n",
    "\n",
    "The questions are repeated here, for clarity of what is demanded. If it does not fit your style to quote them verbatim, change the format.\n",
    "\n",
    "#### Question 1, single-node validation performance\n",
    "What is the performance you get on the validation dataset?\n",
    "\n",
    "#### Question 2, improving training performance\n",
    "(a) How many hidden nodes do you need to reach an accuracy >95% on your training dataset?<br>\n",
    "(b) What is the performance on the validation data set?\n",
    "\n",
    "#### Question 3, optimizing validation performance\n",
    "(a) What is the optimal number of hidden nodes for the syn2 dataset in order to maximize your validation performance?<br>\n",
    "(b) Try to give an explanation for the number you obtained.\n",
    "\n",
    "#### Question 4, optimizing regression performance\n",
    "How many hidden nodes gives the best validation performance?<br>\n",
    "What is the best validation MSE (or correlation coefficient) you get?\n",
    "\n",
    "#### Question 5, improving generalization with regularization\n",
    "(a) Present your optimal model (L2 value and number of hidden nodes) and the validation performance.<br>\n",
    "(b) Do you obtain a better result compared to Q4?\n",
    "\n",
    "#### Question 6, improving generalization with dropout\n",
    "(a) Present your optimal model (dropout value and number of hidden nodes) and the validation performance.<br>\n",
    "(b) Do you obtain a better result compared to Q4/Q5?\n",
    "\n",
    "#### Question 7, create MLP to solve the vowel problem\n",
    "(a) Present an MLP with associated hyperparameters that maximizes the validation performance, and state the training, validation and test performance you obtained.<br>\n",
    "(b) Present your code.\n",
    "\n",
    "#### Question 8, model selection criteria\n",
    "Why can the validation loss increase while the training loss stays constant?\n",
    "Given this situation, what would be your criteria to select the best model?\n",
    "\n",
    "#### Question 9, create MLP for multi-class problem\n",
    "(a) Present an MLP with associated hyperparameters that maximizes the validation performance and give the test performance you obtained.<br>\n",
    "(b) What performance measure(s) do you think is(are) relevant when you select the optimal model for this problem?\n",
    "\n",
    "#### Question 10, minimize network size\n",
    "Train a classifier for the spiral problem with the aim of zero classification error with as small as possible model. Report the model you used.\n",
    "\n",
    "## Summary\n",
    "Connect the summary to your introduction, to provide a brief overview of your findings.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "530px",
    "width": "356.167px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
